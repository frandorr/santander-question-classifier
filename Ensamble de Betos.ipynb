{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El ensamble de Beto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué Beto?\n",
    "\n",
    "Elegí a BETO (BERT entrenado con corpus en castellano, ver notebook de Language Model para más detalle) porque es un modelo que alcanza el estado del arte en diferentes tareas de procesamiento del lenguaje natural. \n",
    "\n",
    "En un principio probé hacer algunos experimentos con FastText, ya que es mucho más rápido de entrenar. Pero los resultados no fueron satisfactorios. Por eso decidí volcarme de lleno a Beto.\n",
    "\n",
    "## Métodos\n",
    "\n",
    "### Preprocesamiento\n",
    "\n",
    "Una de las ventajas de BETO es que no es necesario hacer prácticamente ningún preprocesamiento sobre los datos de entrenamiento. Simplemente alimentando el modelo con lo datos crudos y tokenizando las oraciones es suficiente. Esta es una gran ventaja, incluso hubiera sido más beneficioso aún haber recibido los datos crudos y no con un preprocesamiento como fueron entregados ya que esto puede quitarle poder predictivo.\n",
    "\n",
    "Realicé pruebas haciendo data augmentation sobre el conjunto de entrenamiento, pero no afectaron al score resultante. El método que utilicé fue *completar* la palabra faltante en una oración utilizando el language modeling preentrenado. Con esto generé oraciones extra completando palabra seleccionadas al azar de diferentes oraciones.  \n",
    "\n",
    "### Finetuning Language Model\n",
    "\n",
    "Antes de aplicar un clasificador sobre BETO realicé un finetuning no supervisado (ver notebook de **Language Model**)\n",
    "\n",
    "### Búsqueda de hiperparámetros\n",
    "\n",
    "Para entender el comportamiento de los diferentes parámetros que afectan a BETO, como learning rate, epochs, scheduler, etc, decidí usar MLFlow y loguear las corrida con diferentes parámetros. \n",
    "Basándome en la bibliografía seleccioné una lista de hiperparámetros y lancé múltiples experimentos cruzándolos.\n",
    "\n",
    "Cada cruza de parámetros fue logueada a MLFlow donde pude observar cuales fueron los que mejor comportamiento tuvieron y usarlos para el entrenamiento final. \n",
    "\n",
    "En el siguiente gráfico se pueden ver las curvas de Balanced Accuracy en el conjunto de validación de los mejores 6 modelos obtenidos en la búsqueda de parámetros:\n",
    "\n",
    "![Mejores modelos en Balanced Accuracy](mejores_6.png)\n",
    "\n",
    "### Ensamble\n",
    "\n",
    "Una vez entrenados múltiples modelos con los mejores hiperparámetros conseguidos con MLFlow utilicé un **Voting Hard** sobre las predicciones. Probé diferentes combinaciones de modelos llegando al mejor resultado con 6 BETOs ensamblados.\n",
    "\n",
    "También realicé pruebas haciendo un **voting soft** pero el resultado no mejoró.\n",
    "\n",
    "\n",
    "## Resultado y conclusiones\n",
    "\n",
    "Lo que marcó la diferencia en el puntaje fue haber hecho el ensamble de modelos. Al usar únicamente un modelo con los mejores hiperparámetros el score que había conseguido era de alrededor de 0.85. Esto demuestra el gran poder que tiene el ensamblado. \n",
    "\n",
    "Me hubiera gustado poder hacer un stacking en vez de un voting hard, pero para esto se requiere mucho más tiempo de entrenamiento y cómputo que se hace privativo con un modelo tan pesado como BERT. \n",
    "\n",
    "Esta solución aporta muchísimo en la automatización de preguntas (o cualquier tipo de texto) ahorrando recursos y tiempo. Colocar un modelo como este en producción y a través de una API hacerle consultas no es complicado y las consultas se podrían hacer online y en tiempo real ahorrandole el tiempo a los trabajadores de una trabajo tedioso como es la clasificación de preguntas de forma manual.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código\n",
    "\n",
    "A continuación se encuentra el código utilizado para el entrenamiento de BETO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import mlflow\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule, get_cosine_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import itertools\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dataset/split_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(dataset).dropna()#pd.read_csv('dataset/split_train.csv').dropna()\n",
    "val = pd.read_csv('dataset/split_val.csv').dropna()\n",
    "test = pd.read_csv('dataset/test_santander.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18092, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "train['Intencion'] = train.Intencion.apply(lambda x: int(x.replace('Cat_','')))\n",
    "val['Intencion'] = val.Intencion.apply(lambda x: int(x.replace('Cat_','')))\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(train.Intencion.unique())\n",
    "\n",
    "train['Label'] = train.Intencion.apply(lambda x: le.transform([x])[0])\n",
    "val['Label'] = val.Intencion.apply(lambda x: le.transform([x])[0])\n",
    "\n",
    "sentences_train = train.Pregunta.values\n",
    "labels_train = train.Label.values\n",
    "\n",
    "sentences_val = val.Pregunta.values\n",
    "labels_val = val.Label.values\n",
    "\n",
    "test_sentences = test.Pregunta.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, scheduler, train_dataloader, mlflow_log=True):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 10 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update optimizer and scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    \n",
    "    if mlflow_log:\n",
    "        mlflow.log_metric('Train Loss', avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "def val_step(model, validation_dataloader, mlflow_log=True):\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        y_true+=labels_flat.tolist()\n",
    "        y_pred+=pred_flat.tolist()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)    \n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    print(\"Balanced Accuracy: {0:.2f}\".format(bal_acc))\n",
    "    print(\"Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    if mlflow_log:\n",
    "\n",
    "        mlflow.log_metric(\"Balanced Acc\", bal_acc)\n",
    "        mlflow.log_metric(\"Acc\", avg_val_accuracy)\n",
    "\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training(pretrained, sentences_train, labels_train, sentences_val,\n",
    "                     labels_val, epochs, batch_size,\n",
    "                     max_len, warm_up, lr, freeze=False):\n",
    "    \n",
    "    num_labels = len(np.unique(labels_train))\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained)\n",
    "\n",
    "    for sent in sentences_train:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_len,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels_train)\n",
    "\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    print('Original: ', sentences_train[0])\n",
    "    print('Token IDs:', input_ids[0])\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    # The same for validation:\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences_val:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_len,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels_val)\n",
    "\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    print('Original val: ', sentences_val[0])\n",
    "    print('Token IDs: val', input_ids[0])\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    val_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    print('{:>5,} training samples'.format(len(sentences_train)))\n",
    "    print('{:>5,} validation samples'.format(len(sentences_val)))\n",
    "\n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        pretrained, \n",
    "        num_labels = num_labels,  \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = lr\n",
    "                    )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warm_up,\n",
    "                                                num_training_steps = total_steps)\n",
    "    \n",
    "    return model, optimizer, scheduler, train_dataloader, validation_dataloader\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18092, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda de hiperparámetros con MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 0.00018, 48, 1000\n",
      "Original:  quiero obtener una supercuenta. se puede hacer a trabves d ela web\n",
      "Token IDs: tensor([    4,  1937,  4251,  1108,  1843, 11172,  1009,  1062,  1499,  1409,\n",
      "         1013,  1211, 30946,  2347,  1116,  1040, 30932,  1004,  3953,     5,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "Original val:  una consulta sobre debito desde mi cuenta\n",
      "Token IDs: val tensor([   4, 1108, 6905, 1269, 5452, 1071, 1668, 1153, 1971,    5,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
      "18,092 training samples\n",
      "2,011 validation samples\n",
      "\n",
      "======== Epoch 1 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:43.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:05.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:27.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:50.\n",
      "\n",
      "  Average training loss: 3.80\n",
      "  Training epcoh took: 0:02:04\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.27\n",
      "Accuracy: 0.61\n",
      "  Validation Loss: 1.99\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 2 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:23.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 1.49\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.51\n",
      "Accuracy: 0.73\n",
      "  Validation Loss: 1.17\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 3 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:23.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.86\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.66\n",
      "Accuracy: 0.79\n",
      "  Validation Loss: 0.87\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 4 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:23.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.71\n",
      "Accuracy: 0.81\n",
      "  Validation Loss: 0.75\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 5 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.74\n",
      "Accuracy: 0.83\n",
      "  Validation Loss: 0.71\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 6 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:23.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:53.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.76\n",
      "Accuracy: 0.83\n",
      "  Validation Loss: 0.70\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 7 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:23.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:53.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.79\n",
      "Accuracy: 0.84\n",
      "  Validation Loss: 0.71\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 8 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.78\n",
      "Accuracy: 0.85\n",
      "  Validation Loss: 0.67\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 9 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:23.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.82\n",
      "Accuracy: 0.86\n",
      "  Validation Loss: 0.67\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 10 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.82\n",
      "Accuracy: 0.87\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 11 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:02:07\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.85\n",
      "Accuracy: 0.89\n",
      "  Validation Loss: 0.57\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 12 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:02:06\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.86\n",
      "Accuracy: 0.90\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 13 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:51.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:02:06\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.87\n",
      "Accuracy: 0.90\n",
      "  Validation Loss: 0.57\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 14 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:51.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:02:05\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.85\n",
      "Accuracy: 0.89\n",
      "  Validation Loss: 0.60\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 15 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:50.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:02:05\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.86\n",
      "Accuracy: 0.90\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 16 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:50.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:02:05\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.86\n",
      "Accuracy: 0.90\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 17 / 18 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:51.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:02:05\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.87\n",
      "Accuracy: 0.90\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 18 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    566.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    566.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    566.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    566.    Elapsed: 0:01:50.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:02:05\n",
      "\n",
      "Running Validation...\n",
      "Balanced Accuracy: 0.87\n",
      "Accuracy: 0.90\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n",
      "training with 0.00018, 48, 2000\n",
      "Original:  quiero obtener una supercuenta. se puede hacer a trabves d ela web\n",
      "Token IDs: tensor([    4,  1937,  4251,  1108,  1843, 11172,  1009,  1062,  1499,  1409,\n",
      "         1013,  1211, 30946,  2347,  1116,  1040, 30932,  1004,  3953,     5,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "Original val:  una consulta sobre debito desde mi cuenta\n",
      "Token IDs: val tensor([   4, 1108, 6905, 1269, 5452, 1071, 1668, 1153, 1971,    5,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
      "18,092 training samples\n",
      "2,011 validation samples\n",
      "\n",
      "======== Epoch 1 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    566.    Elapsed: 0:00:22.\n"
     ]
    }
   ],
   "source": [
    "remote_server_uri = \"http://localhost:5000\" # set to your server URI\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(\"/beto_pretrained_lm_20\")\n",
    "\n",
    "epochs = 18\n",
    "batch_size = 32\n",
    "max_len = 48\n",
    "pretrained = './BETOcased-20-epochs/'\n",
    "num_labels = len(train.Label.unique())\n",
    "\n",
    "\n",
    "for lr, warm_up in itertools.product([1.8e-4, 1.7e-4,1.6e-4], [1000, 2000]):\n",
    "    with mlflow.start_run():\n",
    "        print(f\"training with {lr}, {max_len}, {warm_up}\")\n",
    "        mlflow.log_param('lr', lr)\n",
    "        mlflow.log_param('max_len', max_len)\n",
    "        mlflow.log_param('warm_up', warm_up)\n",
    "        mlflow.log_param('model', pretrained)\n",
    "        mlflow.log_param('dataset', dataset)\n",
    "        \n",
    "        train_objects = prepare_training(pretrained, \n",
    "                                         sentences_train,labels_train, \n",
    "                                         sentences_val, labels_val,\n",
    "                                         epochs=epochs, \n",
    "                                         batch_size=batch_size,\n",
    "                                         max_len=max_len,\n",
    "                                         warm_up=warm_up,\n",
    "                                         lr=lr, \n",
    "                                         freeze=False)\n",
    "        model = train_objects[0]\n",
    "        optimizer = train_objects[1]\n",
    "        scheduler = train_objects[2]\n",
    "        train_dataloader = train_objects[3]\n",
    "        validation_dataloader = train_objects[4]\n",
    "        \n",
    "        for epoch_i in range(0, epochs):\n",
    "            train_step(model, optimizer, scheduler, train_dataloader)\n",
    "            val_step(model,validation_dataloader)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sentences = np.array(sentences_train.tolist() + sentences_val.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([110, 216, 129, ...,  34, 294, 234])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_labels = np.concatenate((labels_train, labels_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20103, 20103, 351)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_sentences), len(full_labels), len(np.unique(full_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "c = list(zip(full_sentences, full_labels))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "full_sentences, full_labels = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  no me deja extraer dolares por cajero lo dice que no se puede realizar la transacción\n",
      "Token IDs: tensor([    4,  1084,  1129,  5113, 17522, 28224,  1096,  1285,  3821,  1114,\n",
      "         2429,  1038,  1084,  1062,  1499,  3335,  1030, 17590,     5,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "Original val:  no me deja extraer dolares por cajero lo dice que no se puede realizar la transacción\n",
      "Token IDs: val tensor([    4,  1084,  1129,  5113, 17522, 28224,  1096,  1285,  3821,  1114,\n",
      "         2429,  1038,  1084,  1062,  1499,  3335,  1030, 17590,     5,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "20,103 training samples\n",
      "20,103 validation samples\n",
      "\n",
      "======== Epoch 1 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:21.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:43.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:04.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:26.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:48.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:09.\n",
      "\n",
      "  Average training loss: 3.61\n",
      "  Training epcoh took: 0:02:16\n",
      "\n",
      "======== Epoch 2 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:50.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:12.\n",
      "\n",
      "  Average training loss: 1.38\n",
      "  Training epcoh took: 0:02:19\n",
      "\n",
      "======== Epoch 3 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:51.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:13.\n",
      "\n",
      "  Average training loss: 0.79\n",
      "  Training epcoh took: 0:02:20\n",
      "\n",
      "======== Epoch 4 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:14.\n",
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 5 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:15.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 6 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:15.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 7 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:15.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 8 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:15.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 9 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:15.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 10 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:15.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 11 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:30.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:14.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:02:21\n",
      "\n",
      "======== Epoch 12 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:45.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:14.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:02:20\n",
      "\n",
      "======== Epoch 13 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:51.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:13.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:02:19\n",
      "\n",
      "======== Epoch 14 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:07.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:29.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:51.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:13.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:02:19\n",
      "\n",
      "======== Epoch 15 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:51.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:13.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:02:19\n",
      "\n",
      "======== Epoch 16 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:50.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:13.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:02:19\n",
      "\n",
      "======== Epoch 17 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:50.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:12.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:02:19\n",
      "\n",
      "======== Epoch 18 / 18 ========\n",
      "Training...\n",
      "  Batch   100  of    629.    Elapsed: 0:00:22.\n",
      "  Batch   200  of    629.    Elapsed: 0:00:44.\n",
      "  Batch   300  of    629.    Elapsed: 0:01:06.\n",
      "  Batch   400  of    629.    Elapsed: 0:01:28.\n",
      "  Batch   500  of    629.    Elapsed: 0:01:50.\n",
      "  Batch   600  of    629.    Elapsed: 0:02:12.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:02:18\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "remote_server_uri = \"http://localhost:5000\" # set to your server URI\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(\"/beto_pretrained_lm_20\")\n",
    "\n",
    "epochs = 18\n",
    "batch_size = 32\n",
    "max_len = 48\n",
    "pretrained = './BETOcased-20-epochs/'\n",
    "num_labels = len(np.unique(full_labels))\n",
    "lr = 0.00018\n",
    "warm_up = 1000\n",
    "\n",
    "train_objects = prepare_training(pretrained, \n",
    "                                 full_sentences, full_labels,\n",
    "                                 full_sentences,full_labels, \n",
    "                                 epochs=epochs, \n",
    "                                 batch_size=batch_size,\n",
    "                                 max_len=max_len,\n",
    "                                 warm_up=warm_up,\n",
    "                                 lr=lr, \n",
    "                                 freeze=False)\n",
    "model = train_objects[0]\n",
    "optimizer = train_objects[1]\n",
    "scheduler = train_objects[2]\n",
    "train_dataloader = train_objects[3]\n",
    "#validation_dataloader = train_objects[4]\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    train_step(model, optimizer, scheduler, train_dataloader)\n",
    "    #val_step(model,validation_dataloader)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/beto_lm-20_lr-0.00018_wu-1000_epochs-18'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f'models/beto_lm-20_lr-{lr}_wu-{warm_up}_epochs-{epochs}'\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['querer saber tarjeta sin limite',\n",
       "       '¿cuál es el límite de mi tarjeta santander?',\n",
       "       'hay beneficios en restaurantes de la costa atlántica?', ...,\n",
       "       'quiero pagar de mi open credit un poquito mas del minimo',\n",
       "       'nesecito imprimir mi resumen tarjeta de credito va',\n",
       "       'quiero obtener una visa credito'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  querer saber tarjeta sin limite\n",
      "Token IDs: tensor([    4,  9312,  2486,  7929,  1477, 28290,     5,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "6702 6702\n"
     ]
    }
   ],
   "source": [
    " # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "batch_size = 32\n",
    "max_len = 48\n",
    "pretrained = './BETOcased-20-epochs/'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained)\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', test_sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "\n",
    "print(len(input_ids), len(attention_masks))\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "test_dataset = TensorDataset(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "                    test_dataset, # The validation samples.\n",
    "                    sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "                    batch_size = 128 # Evaluate with this batch size.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('dataset/test_santander.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model_paths = ['models/beto_lm-20_lr-0.00018_wu-1000_epochs-18',\n",
    "                'models/beto_lm-20_lr-0.0001_wu-1000_epochs-18/',\n",
    "               'models/beto_lm-20_lr-0.00015_wu-1000_epochs-18',\n",
    "              'models/beto_lm-20_lr-0.00015_wu-2000_epochs-18',\n",
    "              'models/beto_lm-20_lr-0.0001_wu-2000_epochs-18']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba Voting Soft (no mejoró)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 6,702 test sentences...\n",
      "DONE.\n",
      "Predicting labels for 6,702 test sentences...\n",
      "DONE.\n",
      "Predicting labels for 6,702 test sentences...\n",
      "DONE.\n",
      "Predicting labels for 6,702 test sentences...\n",
      "DONE.\n",
      "Predicting labels for 6,702 test sentences...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "total_preds = []\n",
    "for pretrained in model_paths:\n",
    "    print('Predicting labels for {:,} test sentences...'.format(len(test_dataloader.dataset)))\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        pretrained, \n",
    "        num_labels = num_labels,  \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions = []\n",
    "\n",
    "    # Predict \n",
    "    for batch in test_dataloader:\n",
    "      # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "      # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "          outputs = model(b_input_ids, token_type_ids=None, \n",
    "                          attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    print('DONE.')\n",
    "    # Combine the results across all batches. \n",
    "    flat_predictions = softmax(torch.Tensor(np.concatenate(predictions, axis=0)))\n",
    "    total_preds.append(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_preds = torch.stack(total_preds).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft_vot = np.mean(total_preds,axis=0)\n",
    "flat_predictions_bin = np.argmax(total_preds, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([294, 294, 302, ..., 190, 129, 328])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_predictions_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 6,702 test sentences...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "total_preds = []\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_dataloader.dataset)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "\n",
    "print('DONE.')\n",
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_vot = np.max(total_preds,axis=0)\n",
    "flat_predictions_bin = np.argmax(soft_vot, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions_bin = np.argmax(flat_predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([294, 294, 302, ..., 190, 129, 328])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_predictions_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = le.inverse_transform(flat_predictions_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Intencion'] = res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Intencion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>1549</td>\n",
       "      <td>queria saber cómo conseguir mi iban</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>1832</td>\n",
       "      <td>cuál es el iban del banco xxxxx? (del exterior)</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>2636</td>\n",
       "      <td>donde cargo el swift?</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4402</th>\n",
       "      <td>4402</td>\n",
       "      <td>necesito toda la info de abba swift code etc</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>5791</td>\n",
       "      <td>cuál es el aba del banco xxxxx? (del exterior)</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>6442</td>\n",
       "      <td>quiero saber mi número de c b u</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                         Pregunta  Intencion\n",
       "1549  1549              queria saber cómo conseguir mi iban         34\n",
       "1832  1832  cuál es el iban del banco xxxxx? (del exterior)         34\n",
       "2636  2636                            donde cargo el swift?         34\n",
       "4402  4402     necesito toda la info de abba swift code etc         34\n",
       "5791  5791   cuál es el aba del banco xxxxx? (del exterior)         34\n",
       "6442  6442                  quiero saber mi número de c b u         34"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.Intencion == 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/beto_lm-20_lr-0.00018_wu-1000_epochs-18'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-ae658548666c>:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  pd.Series(res_test).to_csv(f'{model_path}.csv', header=None)\n"
     ]
    }
   ],
   "source": [
    "pd.Series(res_test).to_csv(f'{model_path}.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best voting: 1,2,3, 6,7,8\n",
    "\n",
    "test_1 = pd.read_csv('beto_aug_2.csv', header=None)\n",
    "test_2 = pd.read_csv('beto_best_64_mlflow.csv',header=None)\n",
    "test_3 = pd.read_csv('beto_best_mlflow.csv',header=None)\n",
    "test_4 = pd.read_csv('beto_test_20_epochs.csv',header=None)\n",
    "test_5 = pd.read_csv('beto_uncased_test_21_epochs.csv',header=None)\n",
    "test_6 = pd.read_csv('models/beto_lm-20_lr-0.0001_wu-2000_epochs-18.csv', header=None)\n",
    "test_7 = pd.read_csv('models/beto_lm-20_lr-0.00015_wu-2000_epochs-18.csv', header=None)\n",
    "test_8 = pd.read_csv('models/beto_lm-20_lr-0.00015_wu-1000_epochs-18.csv', header=None)\n",
    "test_9 = pd.read_csv('models/beto_lm-20_lr-0.0001_wu-1000_epochs-18.csv', header=None)\n",
    "test_10 = pd.read_csv('models/beto_lm-20_lr-0.00018_wu-1000_epochs-18.csv', header=None)\n",
    "test_11 = pd.read_csv('voting_best_6.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.stack((test_1[1],test_2[1],test_3[1],test_4[1],test_5[1],test_6[1])),axis=0,return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked_preds = np.stack((test_1[1],test_2[1],test_3[1],test_4[1],test_5[1],test_6[1]))\n",
    "stacked_preds = np.stack((test_11[1], test_9[1],test_10[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "res = []\n",
    "for i in range(stacked_preds.shape[1]):\n",
    "    res.append(Counter(stacked_preds[:,i]).most_common()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-118-40fa1b40ea11>:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  pd.Series(res).to_csv('voting_best_6_and_9-10.csv', header=None)\n"
     ]
    }
   ],
   "source": [
    "pd.Series(res).to_csv('voting_best_6_and_9-10.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, counts = np.unique(res,return_counts=True)\n",
    "train_val, train_counts = np.unique(full_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(res_test, test_6[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
